{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ATPA 2.6 - Missing and Extreme Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 1: Prepares the flight data set for a permutation test\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "\n",
    "flights = pd.read_csv(\"Data/flights.csv\")\n",
    "\n",
    "# transforms the time variables to minutes from midnight. \n",
    "flights = flights.assign(dep_time= flights.dep_time-40*np.floor(flights.dep_time/100), sched_dep_time = flights.sched_dep_time-40*np.floor(flights.sched_dep_time/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dep_time</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>dep_delay</th>\n",
       "      <th>arr_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>carrier</th>\n",
       "      <th>flight</th>\n",
       "      <th>tailnum</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>air_time</th>\n",
       "      <th>distance</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>time_hour</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>317.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>830.0</td>\n",
       "      <td>819</td>\n",
       "      <td>11.0</td>\n",
       "      <td>UA</td>\n",
       "      <td>1545</td>\n",
       "      <td>N14228</td>\n",
       "      <td>EWR</td>\n",
       "      <td>IAH</td>\n",
       "      <td>227.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2013-01-01 05:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>333.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>830</td>\n",
       "      <td>20.0</td>\n",
       "      <td>UA</td>\n",
       "      <td>1714</td>\n",
       "      <td>N24211</td>\n",
       "      <td>LGA</td>\n",
       "      <td>IAH</td>\n",
       "      <td>227.0</td>\n",
       "      <td>1416</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>2013-01-01 05:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>342.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>850</td>\n",
       "      <td>33.0</td>\n",
       "      <td>AA</td>\n",
       "      <td>1141</td>\n",
       "      <td>N619AA</td>\n",
       "      <td>JFK</td>\n",
       "      <td>MIA</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1089</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>2013-01-01 05:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n",
       "0  2013      1    1     317.0           315.0        2.0     830.0   \n",
       "1  2013      1    1     333.0           329.0        4.0     850.0   \n",
       "2  2013      1    1     342.0           340.0        2.0     923.0   \n",
       "\n",
       "   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n",
       "0             819       11.0      UA    1545  N14228    EWR  IAH     227.0   \n",
       "1             830       20.0      UA    1714  N24211    LGA  IAH     227.0   \n",
       "2             850       33.0      AA    1141  N619AA    JFK  MIA     160.0   \n",
       "\n",
       "   distance  hour  minute            time_hour        Date  \n",
       "0      1400     5      15  2013-01-01 05:00:00  2013-01-01  \n",
       "1      1416     5      29  2013-01-01 05:00:00  2013-01-01  \n",
       "2      1089     5      40  2013-01-01 05:00:00  2013-01-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    328521.000000\n",
       "mean        822.168169\n",
       "std         292.730990\n",
       "min           1.000000\n",
       "25%         547.000000\n",
       "50%         841.000000\n",
       "75%        1064.000000\n",
       "max        1440.000000\n",
       "Name: dep_time, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(flights.head(3))\n",
    "flights.dep_time.describe() # recall that dep_time is hhmm for the time past midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                 0\n",
       "month                0\n",
       "day                  0\n",
       "dep_time          8255\n",
       "sched_dep_time       0\n",
       "dep_delay         8255\n",
       "arr_time          8713\n",
       "sched_arr_time       0\n",
       "arr_delay         9430\n",
       "carrier              0\n",
       "flight               0\n",
       "tailnum           2512\n",
       "origin               0\n",
       "dest                 0\n",
       "air_time          9430\n",
       "distance             0\n",
       "hour                 0\n",
       "minute               0\n",
       "time_hour            0\n",
       "Date                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.apply(lambda col: col.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dep_time</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>dep_delay</th>\n",
       "      <th>arr_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>carrier</th>\n",
       "      <th>flight</th>\n",
       "      <th>tailnum</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>air_time</th>\n",
       "      <th>distance</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>time_hour</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>317.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>830.0</td>\n",
       "      <td>819</td>\n",
       "      <td>11.0</td>\n",
       "      <td>UA</td>\n",
       "      <td>1545</td>\n",
       "      <td>N14228</td>\n",
       "      <td>EWR</td>\n",
       "      <td>IAH</td>\n",
       "      <td>227.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>2013-01-01 05:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>333.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>830</td>\n",
       "      <td>20.0</td>\n",
       "      <td>UA</td>\n",
       "      <td>1714</td>\n",
       "      <td>N24211</td>\n",
       "      <td>LGA</td>\n",
       "      <td>IAH</td>\n",
       "      <td>227.0</td>\n",
       "      <td>1416</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>2013-01-01 05:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>342.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>850</td>\n",
       "      <td>33.0</td>\n",
       "      <td>AA</td>\n",
       "      <td>1141</td>\n",
       "      <td>N619AA</td>\n",
       "      <td>JFK</td>\n",
       "      <td>MIA</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1089</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>2013-01-01 05:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>344.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>1022</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>B6</td>\n",
       "      <td>725</td>\n",
       "      <td>N804JB</td>\n",
       "      <td>JFK</td>\n",
       "      <td>BQN</td>\n",
       "      <td>183.0</td>\n",
       "      <td>1576</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>2013-01-01 05:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>812.0</td>\n",
       "      <td>837</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>DL</td>\n",
       "      <td>461</td>\n",
       "      <td>N668DN</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>116.0</td>\n",
       "      <td>762</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01 06:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>354.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>728</td>\n",
       "      <td>12.0</td>\n",
       "      <td>UA</td>\n",
       "      <td>1696</td>\n",
       "      <td>N39463</td>\n",
       "      <td>EWR</td>\n",
       "      <td>ORD</td>\n",
       "      <td>150.0</td>\n",
       "      <td>719</td>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>2013-01-01 05:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>355.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>854</td>\n",
       "      <td>19.0</td>\n",
       "      <td>B6</td>\n",
       "      <td>507</td>\n",
       "      <td>N516JB</td>\n",
       "      <td>EWR</td>\n",
       "      <td>FLL</td>\n",
       "      <td>158.0</td>\n",
       "      <td>1065</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01 06:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>357.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>723</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>EV</td>\n",
       "      <td>5708</td>\n",
       "      <td>N829AS</td>\n",
       "      <td>LGA</td>\n",
       "      <td>IAD</td>\n",
       "      <td>53.0</td>\n",
       "      <td>229</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01 06:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>846</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>B6</td>\n",
       "      <td>79</td>\n",
       "      <td>N593JB</td>\n",
       "      <td>JFK</td>\n",
       "      <td>MCO</td>\n",
       "      <td>140.0</td>\n",
       "      <td>944</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01 06:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>827.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>753.0</td>\n",
       "      <td>745</td>\n",
       "      <td>8.0</td>\n",
       "      <td>AA</td>\n",
       "      <td>301</td>\n",
       "      <td>N3ALAA</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ORD</td>\n",
       "      <td>138.0</td>\n",
       "      <td>733</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01 06:00:00</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n",
       "0  2013      1    1     317.0           315.0        2.0     830.0   \n",
       "1  2013      1    1     333.0           329.0        4.0     850.0   \n",
       "2  2013      1    1     342.0           340.0        2.0     923.0   \n",
       "3  2013      1    1     344.0           345.0       -1.0    1004.0   \n",
       "4  2013      1    1    1026.0           360.0       -6.0     812.0   \n",
       "5  2013      1    1     354.0           358.0       -4.0     740.0   \n",
       "6  2013      1    1     355.0           360.0       -5.0     913.0   \n",
       "7  2013      1    1     357.0           360.0       -3.0     709.0   \n",
       "8  2013      1    1       NaN           360.0       -3.0     838.0   \n",
       "9  2013      1    1     827.0           360.0       -2.0     753.0   \n",
       "\n",
       "   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n",
       "0             819       11.0      UA    1545  N14228    EWR  IAH     227.0   \n",
       "1             830       20.0      UA    1714  N24211    LGA  IAH     227.0   \n",
       "2             850       33.0      AA    1141  N619AA    JFK  MIA     160.0   \n",
       "3            1022      -18.0      B6     725  N804JB    JFK  BQN     183.0   \n",
       "4             837      -25.0      DL     461  N668DN    LGA  ATL     116.0   \n",
       "5             728       12.0      UA    1696  N39463    EWR  ORD     150.0   \n",
       "6             854       19.0      B6     507  N516JB    EWR  FLL     158.0   \n",
       "7             723      -14.0      EV    5708  N829AS    LGA  IAD      53.0   \n",
       "8             846       -8.0      B6      79  N593JB    JFK  MCO     140.0   \n",
       "9             745        8.0      AA     301  N3ALAA    LGA  ORD     138.0   \n",
       "\n",
       "   distance  hour  minute            time_hour        Date  \n",
       "0      1400     5      15  2013-01-01 05:00:00  2013-01-01  \n",
       "1      1416     5      29  2013-01-01 05:00:00  2013-01-01  \n",
       "2      1089     5      40  2013-01-01 05:00:00  2013-01-01  \n",
       "3      1576     5      45  2013-01-01 05:00:00  2013-01-01  \n",
       "4       762     6       0  2013-01-01 06:00:00  2013-01-01  \n",
       "5       719     5      58  2013-01-01 05:00:00  2013-01-01  \n",
       "6      1065     6       0  2013-01-01 06:00:00  2013-01-01  \n",
       "7       229     6       0  2013-01-01 06:00:00  2013-01-01  \n",
       "8       944     6       0  2013-01-01 06:00:00  2013-01-01  \n",
       "9       733     6       0  2013-01-01 06:00:00  2013-01-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights = flights.loc[:9, :]\n",
    "flights.loc[8, 'dep_time'] = np.nan\n",
    "display(flights)\n",
    "flights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49511606910025646"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_missing = 'dep_time'\n",
    "col_base = 'sched_dep_time'\n",
    "\n",
    "n = 100\n",
    "R = np.zeros(n)\n",
    "missing = flights.loc[:, col_missing].isna()\n",
    "T = flights.loc[missing, col_base].mean() - flights.loc[~missing, col_base].mean()\n",
    "\n",
    "for i in range(n):\n",
    "    samp_i = flights.loc[:, col_missing].sample(frac=1., ignore_index=True)\n",
    "    missing_i = samp_i.isna()\n",
    "    R[i] = flights.loc[missing_i, col_base].mean() - flights.loc[~missing_i, col_base].mean()\n",
    "\n",
    "R.mean()\n",
    "np.quantile(R, [0.025, 0.5, 0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Permutation_Test:\n",
    "    def __init__(self, data, col_missing, col_base):\n",
    "        self.data = data\n",
    "        self.col_missing = col_missing\n",
    "        self.col_base = col_base\n",
    "    \n",
    "    def by_hand(self, quantile_lower, quantile_upper, n):\n",
    "        missing = self.data.loc[:, self.col_missing].isna()\n",
    "        T = self.data.loc[missing, self.col_base].mean() - self.data.loc[~missing, self.col_base].mean()\n",
    "\n",
    "        R = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            samp_i = self.data.loc[:, self.col_missing].sample(frac=1., ignore_index=True)\n",
    "            missing_i = samp_i.isna()\n",
    "            R[i] = self.data.loc[missing_i, self.col_base].mean() - self.data.loc[~missing_i, self.col_base].mean()\n",
    "\n",
    "        return {'T': np.round(T, 3),\n",
    "                'RMean': np.round(R.mean(), 3),\n",
    "                'RQuantiles': np.round(np.quantile(R, [quantile_lower, quantile_upper]), 3),\n",
    "                'T_InQuantiles': (T > quantile_lower) & (T < quantile_upper)}\n",
    "\n",
    "    def normal_approximation(self, quantile_percent):\n",
    "        missing = self.data.loc[:, self.col_missing].isna()\n",
    "        T = self.data.loc[missing, self.col_base].mean() - self.data.loc[~missing, self.col_base].mean()\n",
    "\n",
    "        n = self.data.shape[0]\n",
    "        m = self.data.loc[:,self.col_missing].isna().sum()\n",
    "        v = np.var(self.data.loc[:, self.col_base])\n",
    "        mean_R = 0\n",
    "        var_R = n * v / (m*(n-m))\n",
    "\n",
    "        u_bound = norm.ppf(quantile_percent, loc=mean_R, scale=np.sqrt(var_R))\n",
    "\n",
    "        return {'T': np.round(T, 3),\n",
    "                'RMean': mean_R,\n",
    "                'RVar': np.round(var_R, 3),\n",
    "                'RQuantiles': np.array([-u_bound, u_bound]),\n",
    "                'T_InQuantiles': (T > -u_bound) & (T < u_bound)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T': 90.782,\n",
       " 'RMean': 1.575,\n",
       " 'RQuantiles': array([-2.961,  4.606]),\n",
       " 'T_InQuantiles': False}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm = Permutation_Test(data=flights, \n",
    "                        col_missing='dep_time', \n",
    "                        col_base='sched_dep_time')\n",
    "\n",
    "perm.by_hand(quantile_lower=0.025,\n",
    "             quantile_upper=0.975,\n",
    "             n=1000)\n",
    "\n",
    "perm.normal_approximation(quantile_percent=0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunk 2: Performs a permutation test for the dep_time variable in the flights data set\n",
    "\n",
    "# Find the test statistic for the data\n",
    "is_missing = flights.dep_time.isnull()\n",
    "Test_stat = flights.sched_dep_time.loc[is_missing].mean() - flights.sched_dep_time.loc[-is_missing].mean()\n",
    "Test_stat\n",
    "\n",
    "# Do a for loop to reorder the data, find the R values, and create a distribution to compare against the test statistic. The permutations are done on the variable that does not have missing value. That way the is_missing vector can be reused.\n",
    "random.seed(1234)\n",
    "nrun = 1000\n",
    "R_vals = [0 for i in range(nrun)]\n",
    "ind = np.arange(len(flights))\n",
    "for i in range(nrun):\n",
    "  np.random.shuffle(ind)\n",
    "  flights[\"temp_sched\"] = flights.sched_dep_time[ind].reset_index().iloc[:,1]\n",
    "  R_vals[i] =  flights.temp_sched[is_missing==True].mean() - flights.temp_sched.loc[is_missing==False].mean()\n",
    "\n",
    "# Create a confidence interval\n",
    "CI = np.quantile(R_vals,[.025,.975])\n",
    "CI \n",
    "\n",
    "# Check condition\n",
    "Test_stat < CI[1] and Test_stat > CI[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 2A: Perform the permutation test using the normal approximation\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = flights.shape[0]\n",
    "m = is_missing.sum()\n",
    "v = flights.sched_dep_time.var()\n",
    "var_R = n*v/m/(n-m)\n",
    "CI2 = 1.96 * np.sqrt(var_R)\n",
    "CI2\n",
    "np.abs(Test_stat) < CI2\n",
    "\n",
    "\n",
    "# The following are some additional checks on the approximation\n",
    "# Calculate the variance of the sample of R-values and compare to the variance based on all permutations\n",
    "np.var(R_vals)\n",
    "var_R\n",
    "\n",
    "# Make a simple histogram of the R-values\n",
    "plt.hist(R_vals)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "# In this case the approximation worked well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 3: Creates the production data set\n",
    "\n",
    "# Create Data Set\n",
    "production = pd.DataFrame({\"Produced\": [145,212,137,187,166],\"Employees\": [6,8,6,7,7],\"Available_Machines\":[19,24,np.nan,20,18],\"Hours_Open\":[10,9,8,9,6]})\n",
    "production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 4: Mean imputation\n",
    "\n",
    "# using SingleImputer from the autoimpute package\n",
    "# Warnings produced, if any, can be ignored\n",
    "\n",
    "from autoimpute.imputations import SingleImputer, MultipleImputer\n",
    "\n",
    "imp = SingleImputer(strategy={\"Available_Machines\":'mean'})\n",
    "production_mean = imp.fit_transform(production)\n",
    "production_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 5: Regression imputation\n",
    "\n",
    "# using SingleImputer from the autoimpute package\n",
    "\n",
    "imp = MultipleImputer(n=1,return_list=True,strategy={\"Available_Machines\":'least squares'})\n",
    "# you can set n to a higher value if there is randomness in the imputation procedure and you want to see multiple versions, however, you do not need to \n",
    "# return_list=True is there so we can see the output\n",
    "reg_imp = imp.fit_transform(production.drop(columns=[\"Produced\"]))[0][1] # The output creates a nested structure and [0][1] helps us get the data frame\n",
    "reg_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 5-optional: Do the above imputation manually\n",
    "\n",
    "# Perform the regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "production_clean = production.loc[~pd.isna(production.Available_Machines),:]\n",
    "\n",
    "X = production_clean.iloc[:,[1,3]].values.reshape(-1,2)\n",
    "Y = production_clean.iloc[:,2].values.reshape(-1,1)\n",
    "reg = LinearRegression().fit(X,Y)\n",
    "reg.predict(np.array([[6,8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 6: K Nearest Neighbors imputation\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "colnames = production.columns\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "pd.DataFrame(imputer.fit_transform(production.drop(columns=[\"Produced\"])), columns = colnames[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 7: Imputing both categorical and numeric variables\n",
    "\n",
    "production[\"Manager\"] = [\"On\",\"Off\",\"On\",\"Off\",np.nan]\n",
    "\n",
    "imputer = SingleImputer(strategy={\"Manager\":\"categorical\"})\n",
    "data_imputed = imputer.fit_transform(production)\n",
    "data_imputed\n",
    "\n",
    "# when using regression methods with mixed data types you need to binarize the categorical variables\n",
    "newprod = pd.get_dummies(production.drop(columns=[\"Produced\"]),drop_first=True)\n",
    "imputer = MultipleImputer(return_list=True,n=1,strategy={\"Available_Machines\":\"least squares\",\"Manager_On\":\"binary logistic\"})\n",
    "data_imputed = imputer.fit_transform(newprod)[0][1]\n",
    "data_imputed\n",
    "\n",
    "# Note that the two imputation methods do not produce the same results for Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 8: Exercise 2.6.1 Check for Understanding Questions\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Read in autombile data set\n",
    "Auto_Names = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\",\"aspiration\", \"num_doors\", \"body_style\", \"drive_wheels\",\"engine_location\", \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\", \"engine_type\",\"num_cylinders\", \"engine_size\", \"fuel_system\", \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\", \"city_mpg\", \"highway_mpg\", \"price\"]\n",
    "\n",
    "auto = pd.read_csv(\"automobile.csv\",names=Auto_Names)\n",
    "cols_to_keep = [\"normalized_losses\",\"num_doors\",\"body_style\",\"curb_weight\",\"engine_type\",\"bore\",\"city_mpg\",\"price\"]\n",
    "auto = auto[cols_to_keep]\n",
    "\n",
    "# Replace ?'s with NAs\n",
    "auto = auto.replace(\"?\",np.nan)\n",
    "# Change these three columns from character to numeric\n",
    "auto = auto.assign(normalized_losses = [float(x) for x in auto.normalized_losses],bore = [float(x) for x in auto.bore],price = [float(x) for x in auto.price])\n",
    "\n",
    "# Examine the normalized_loss variable for missingness at random.\n",
    "# 1. Check a box plot for curb_weight and city_mpg when normalized_loss is and isn't missing\n",
    "\n",
    "# 2. Perform a permutation test for the normalized_loss variable using curb_weight as the variable without missing data.\n",
    "\n",
    "# 3. Explain the missingness of bore\n",
    "\n",
    "# 4. Perform mean imputation for the missing values of price\n",
    "\n",
    "# 5. Perform regression imputation for price\n",
    "\n",
    "# 6. Perform imputation for both price and num_doors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9A: Exercise 2.6.1: Question 1\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Read in autombile data set\n",
    "Auto_Names = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\",\"aspiration\", \"num_doors\", \"body_style\", \"drive_wheels\",\"engine_location\", \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\", \"engine_type\",\"num_cylinders\", \"engine_size\", \"fuel_system\", \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\", \"city_mpg\", \"highway_mpg\", \"price\"]\n",
    "\n",
    "auto = pd.read_csv(\"automobile.csv\",names=Auto_Names)\n",
    "cols_to_keep = [\"normalized_losses\",\"num_doors\",\"body_style\",\"curb_weight\",\"engine_type\",\"bore\",\"city_mpg\",\"price\"]\n",
    "auto = auto[cols_to_keep]\n",
    "\n",
    "# Replace ?'s with NAs\n",
    "auto = auto.replace(\"?\",np.nan)\n",
    "\n",
    "# Convert numeric variables to numeric type\n",
    "auto = auto.assign(normalized_losses = [float(x) for x in auto.normalized_losses],bore = [float(x) for x in auto.bore],price = [float(x) for x in auto.price])\n",
    "\n",
    "# Examine the normalized_losses variable for missingness at random.\n",
    "# 1. Check a box plot for curb_weight and city_mpg when normalized losses is and isn't missing\n",
    "auto[\"missing\"] = auto.normalized_losses.isnull()\n",
    "sns.boxplot(x = \"missing\",y=\"curb_weight\",data=auto)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "sns.boxplot(x = \"missing\",y=\"city_mpg\",data=auto)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "\n",
    "# It seems as if there may be a relationship between missingness and both curb_weight city_mpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9B: Exercise 2.6.1: Question 2\n",
    "\n",
    "# 2. Perform a permutation test for the normalized_loss variable using curb_weight as the variable without missing data.\n",
    "Test_stat = auto.curb_weight[auto.missing].mean()-auto.curb_weight[-auto.missing].mean()\n",
    "Test_stat\n",
    "\n",
    "# Do a for loop to reorder the data, find the R values, and create a distribution to compare against the test statistic\n",
    "random.seed(1234)\n",
    "nrun = 1000\n",
    "R_vals = [0 for i in range(nrun)]\n",
    "ind = np.arange(len(auto))\n",
    "for i in range(nrun):\n",
    "  np.random.shuffle(ind)\n",
    "  auto[\"temp_curb_weight\"] = auto.curb_weight[ind].reset_index().iloc[:,1]\n",
    "  R_vals[i] = auto.temp_curb_weight[auto.missing].mean()-auto.temp_curb_weight[-auto.missing].mean()\n",
    "\n",
    "# Create a confidence interval\n",
    "CI = np.quantile(R_vals,[.025,.975])\n",
    "CI \n",
    "\n",
    "# Check condition\n",
    "Test_stat < CI[1] and Test_stat > CI[0]\n",
    "# normalized_losses are not missing at random\n",
    "auto = auto.drop(columns=[\"missing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9C: Exercise 2.6.1: Question 3\n",
    "\n",
    "# 3. Explain the missingness of bore with relationship to engine_type\n",
    "\n",
    "# Look at the four records where bore is missing\n",
    "pd.set_option('max_columns', None) # This will display all the columns\n",
    "auto.loc[pd.isna(auto.bore),:]\n",
    "\n",
    "# All have engine_type = rotor. Now check that this is the only time rotor appears\n",
    "auto.loc[auto.engine_type == \"rotor\",:]\n",
    "\n",
    "# When bore is missing, those are also the only incidences of the engine_type = rotor.\n",
    "# At this point an investigation of this relationship might be in order. We will elect to delete these records, but that will imply any model we build will not be able to make predictions for cars with that engine_type.\n",
    "\n",
    "auto = auto.loc[~pd.isna(auto.bore),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9D: Exercise 2.6.1: Question 4\n",
    "\n",
    "# 4. Perform mean imputation for the missing values of price. \n",
    "\n",
    "price_missing = auto.price.isnull()\n",
    "\n",
    "imp = SingleImputer(strategy={\"price\":'mean'})\n",
    "auto_mean = imp.fit_transform(auto)\n",
    "auto_mean.loc[price_missing,\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9E: Exercise 2.6.1: Question 5\n",
    "\n",
    "# 5. Perform regression imputation for the price\n",
    "\n",
    "auto_dummies = pd.get_dummies(auto.drop(columns=[\"normalized_losses\"]),drop_first=True)\n",
    "\n",
    "imp = MultipleImputer(n=1,return_list=True,strategy={\"price\":'least squares'})\n",
    "auto_reg = imp.fit_transform(auto_dummies)[0][1]\n",
    "auto_reg.loc[price_missing,\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9F: Exercise 2.6.1: Question 6\n",
    "\n",
    "# 6. Perform imputation for both price and num_doors.\n",
    "\n",
    "doors_missing = auto.num_doors.isnull()\n",
    "\n",
    "imp = MultipleImputer(n=1,return_list=True,strategy={\"price\":'least squares',\"num_doors_two\":'binary logistic'})\n",
    "auto_reg_2 = imp.fit_transform(auto_dummies)[0][1]\n",
    "auto_reg_2.loc[doors_missing,\"num_doors_two\"]\n",
    "auto_reg_2.loc[price_missing,\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 10: Identifying outliers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "hotel_bookings = pd.read_csv(\"hotel_bookings.csv\")\n",
    "\n",
    "# full data boxplot\n",
    "sns.boxplot(y = \"stays_in_week_nights\",data=hotel_bookings)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "\n",
    "# full data histogram\n",
    "sns.displot(hotel_bookings.stays_in_week_nights,binwidth=1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 11: Looking for outliers\n",
    "\n",
    "# Try cutoffs of 3, 5, and 10, \n",
    "\n",
    "cutoff = 5\n",
    "\n",
    "hotel_trimmed = hotel_bookings.assign(zscores = (hotel_bookings.stays_in_week_nights-hotel_bookings.stays_in_week_nights.mean())/hotel_bookings.stays_in_week_nights.std())\n",
    "hotel_trimmed = hotel_trimmed.loc[hotel_trimmed.zscores < cutoff,:]\n",
    "100*(1 - len(hotel_trimmed.index)/len(hotel_bookings.index))\n",
    "sns.displot(hotel_trimmed.stays_in_week_nights,binwidth=1)\n",
    "plt.title(\"Trimmed at Zscores above \" + str(cutoff))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChunkS 12 - 15 Various methods to deal with outliers.\n",
    "# Chunk 12: 1. Log transform\n",
    "Because there are 0's, the transform is log(x+1)\n",
    "\n",
    "hotel_log = hotel_bookings.assign(logstay = np.log(hotel_bookings.stays_in_week_nights+1))\n",
    "sns.displot(hotel_log.logstay,binwidth=.5)\n",
    "plt.title(\"Log Transformation\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 13: 2. Removing outliers. This is the same as in # Chunk 11. The cutoff in this example is a z-score of 5.\n",
    "\n",
    "cutoff = 5\n",
    "hotel_trimmed = hotel_bookings.assign(zscores = (hotel_bookings.stays_in_week_nights-hotel_bookings.stays_in_week_nights.mean())/hotel_bookings.stays_in_week_nights.std())\n",
    "hotel_trimmed = hotel_trimmed.loc[hotel_trimmed.zscores < cutoff,:]\n",
    "sns.displot(hotel_trimmed.stays_in_week_nights,binwidth=1)\n",
    "plt.title(\"Trimmed at Zscores above \" + str(cutoff))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 14: 3. Change the values. In this case we can cap all observations above 6 as being equal to 6. \n",
    "\n",
    "cap =  6\n",
    "hotel_capped = hotel_bookings\n",
    "hotel_capped.stays_in_week_nights = hotel_capped.stays_in_week_nights.clip(upper=cap)\n",
    "sns.displot(hotel_capped.stays_in_week_nights,binwidth=1)\n",
    "plt.title(\"Capped at \"+str(cap))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 15: 4. Replace variable with percentiles.  \n",
    "\n",
    "hotel_bookings = pd.read_csv(\"hotel_bookings.csv\")\n",
    "\n",
    "hotel_bookings[\"perc_stay\"] = hotel_bookings.stays_in_week_nights.rank(pct=True)\n",
    "\n",
    "plt.plot(hotel_bookings.stays_in_week_nights,hotel_bookings.perc_stay,\"o\")\n",
    "plt.xlabel(\"Weeknight Stays\",y=\"Percentiles\")\n",
    "plt.title(\"Percentile Transform by Raw Data\")\n",
    "plt.show() \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 16: Use DBSCAN to find outliers based on several observations. Change eps and minPts to affect the algorithm. The number of outliers are the number of noise points and given a cluster value of -1. \n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "hotel_small = hotel_bookings.loc[:,[\"stays_in_week_nights\",\"stays_in_weekend_nights\",\"adults\"]]\n",
    "hotel_small = hotel_small.loc[0:9999,:]\n",
    "\n",
    "eps = 3\n",
    "minPts = 10\n",
    "X = hotel_small.to_numpy()\n",
    "clustering = DBSCAN(eps = eps, min_samples = minPts).fit(X)\n",
    "cluster = pd.to_numeric(clustering.labels_)\n",
    "cluster = pd.DataFrame(cluster, columns=[\"cluster\"])\n",
    "hotel_small2 = pd.concat([hotel_small,cluster],axis=1)\n",
    "hotel_small2.query(\"cluster < 0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOAModule_Notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e17914ab529f92bc20a1469aaf6b9ace0664154d3788c29b41b731a15dbe8776"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
