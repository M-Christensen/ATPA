{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ATPA 2.6 - Missing and Extreme Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 1: Prepares the flight data set for a permutation test\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "flights = pd.read_csv(\"flights.csv\")\n",
    "\n",
    "# transforms the time variables to minutes from midnight. \n",
    "flights = flights.assign(dep_time= flights.dep_time-40*np.floor(flights.dep_time/100), sched_dep_time = flights.sched_dep_time-40*np.floor(flights.sched_dep_time/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 2: Performs a permutation test for the dep_time variable in the flights data set\n",
    "\n",
    "# Find the test statistic for the data\n",
    "is_missing = flights.dep_time.isnull()\n",
    "Test_stat = flights.sched_dep_time.loc[is_missing].mean() - flights.sched_dep_time.loc[-is_missing].mean()\n",
    "Test_stat\n",
    "\n",
    "# Do a for loop to reorder the data, find the R values, and create a distribution to compare against the test statistic. The permutations are done on the variable that does not have missing value. That way the is_missing vector can be reused.\n",
    "random.seed(1234)\n",
    "nrun = 1000\n",
    "R_vals = [0 for i in range(nrun)]\n",
    "ind = np.arange(len(flights))\n",
    "for i in range(nrun):\n",
    "  np.random.shuffle(ind)\n",
    "  flights[\"temp_sched\"] = flights.sched_dep_time[ind].reset_index().iloc[:,1]\n",
    "  R_vals[i] =  flights.temp_sched[is_missing==True].mean() - flights.temp_sched.loc[is_missing==False].mean()\n",
    "\n",
    "# Create a confidence interval\n",
    "CI = np.quantile(R_vals,[.025,.975])\n",
    "CI \n",
    "\n",
    "# Check condition\n",
    "Test_stat < CI[1] and Test_stat > CI[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 2A: Perform the permutation test using the normal approximation\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = flights.shape[0]\n",
    "m = is_missing.sum()\n",
    "v = flights.sched_dep_time.var()\n",
    "var_R = n*v/m/(n-m)\n",
    "CI2 = 1.96 * np.sqrt(var_R)\n",
    "CI2\n",
    "np.abs(Test_stat) < CI2\n",
    "\n",
    "\n",
    "# The following are some additional checks on the approximation\n",
    "# Calculate the variance of the sample of R-values and compare to the variance based on all permutations\n",
    "np.var(R_vals)\n",
    "var_R\n",
    "\n",
    "# Make a simple histogram of the R-values\n",
    "plt.hist(R_vals)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "# In this case the approximation worked well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 3: Creates the production data set\n",
    "\n",
    "# Create Data Set\n",
    "production = pd.DataFrame({\"Produced\": [145,212,137,187,166],\"Employees\": [6,8,6,7,7],\"Available_Machines\":[19,24,np.nan,20,18],\"Hours_Open\":[10,9,8,9,6]})\n",
    "production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 4: Mean imputation\n",
    "\n",
    "# using SingleImputer from the autoimpute package\n",
    "# Warnings produced, if any, can be ignored\n",
    "\n",
    "from autoimpute.imputations import SingleImputer, MultipleImputer\n",
    "\n",
    "imp = SingleImputer(strategy={\"Available_Machines\":'mean'})\n",
    "production_mean = imp.fit_transform(production)\n",
    "production_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 5: Regression imputation\n",
    "\n",
    "# using SingleImputer from the autoimpute package\n",
    "\n",
    "imp = MultipleImputer(n=1,return_list=True,strategy={\"Available_Machines\":'least squares'})\n",
    "# you can set n to a higher value if there is randomness in the imputation procedure and you want to see multiple versions, however, you do not need to \n",
    "# return_list=True is there so we can see the output\n",
    "reg_imp = imp.fit_transform(production.drop(columns=[\"Produced\"]))[0][1] # The output creates a nested structure and [0][1] helps us get the data frame\n",
    "reg_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 5-optional: Do the above imputation manually\n",
    "\n",
    "# Perform the regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "production_clean = production.loc[~pd.isna(production.Available_Machines),:]\n",
    "\n",
    "X = production_clean.iloc[:,[1,3]].values.reshape(-1,2)\n",
    "Y = production_clean.iloc[:,2].values.reshape(-1,1)\n",
    "reg = LinearRegression().fit(X,Y)\n",
    "reg.predict(np.array([[6,8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 6: K Nearest Neighbors imputation\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "colnames = production.columns\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "pd.DataFrame(imputer.fit_transform(production.drop(columns=[\"Produced\"])), columns = colnames[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 7: Imputing both categorical and numeric variables\n",
    "\n",
    "production[\"Manager\"] = [\"On\",\"Off\",\"On\",\"Off\",np.nan]\n",
    "\n",
    "imputer = SingleImputer(strategy={\"Manager\":\"categorical\"})\n",
    "data_imputed = imputer.fit_transform(production)\n",
    "data_imputed\n",
    "\n",
    "# when using regression methods with mixed data types you need to binarize the categorical variables\n",
    "newprod = pd.get_dummies(production.drop(columns=[\"Produced\"]),drop_first=True)\n",
    "imputer = MultipleImputer(return_list=True,n=1,strategy={\"Available_Machines\":\"least squares\",\"Manager_On\":\"binary logistic\"})\n",
    "data_imputed = imputer.fit_transform(newprod)[0][1]\n",
    "data_imputed\n",
    "\n",
    "# Note that the two imputation methods do not produce the same results for Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 8: Exercise 2.6.1 Check for Understanding Questions\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Read in autombile data set\n",
    "Auto_Names = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\",\"aspiration\", \"num_doors\", \"body_style\", \"drive_wheels\",\"engine_location\", \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\", \"engine_type\",\"num_cylinders\", \"engine_size\", \"fuel_system\", \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\", \"city_mpg\", \"highway_mpg\", \"price\"]\n",
    "\n",
    "auto = pd.read_csv(\"automobile.csv\",names=Auto_Names)\n",
    "cols_to_keep = [\"normalized_losses\",\"num_doors\",\"body_style\",\"curb_weight\",\"engine_type\",\"bore\",\"city_mpg\",\"price\"]\n",
    "auto = auto[cols_to_keep]\n",
    "\n",
    "# Replace ?'s with NAs\n",
    "auto = auto.replace(\"?\",np.nan)\n",
    "# Change these three columns from character to numeric\n",
    "auto = auto.assign(normalized_losses = [float(x) for x in auto.normalized_losses],bore = [float(x) for x in auto.bore],price = [float(x) for x in auto.price])\n",
    "\n",
    "# Examine the normalized_loss variable for missingness at random.\n",
    "# 1. Check a box plot for curb_weight and city_mpg when normalized_loss is and isn't missing\n",
    "\n",
    "# 2. Perform a permutation test for the normalized_loss variable using curb_weight as the variable without missing data.\n",
    "\n",
    "# 3. Explain the missingness of bore\n",
    "\n",
    "# 4. Perform mean imputation for the missing values of price\n",
    "\n",
    "# 5. Perform regression imputation for price\n",
    "\n",
    "# 6. Perform imputation for both price and num_doors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9A: Exercise 2.6.1: Question 1\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Read in autombile data set\n",
    "Auto_Names = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\",\"aspiration\", \"num_doors\", \"body_style\", \"drive_wheels\",\"engine_location\", \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\", \"engine_type\",\"num_cylinders\", \"engine_size\", \"fuel_system\", \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\", \"city_mpg\", \"highway_mpg\", \"price\"]\n",
    "\n",
    "auto = pd.read_csv(\"automobile.csv\",names=Auto_Names)\n",
    "cols_to_keep = [\"normalized_losses\",\"num_doors\",\"body_style\",\"curb_weight\",\"engine_type\",\"bore\",\"city_mpg\",\"price\"]\n",
    "auto = auto[cols_to_keep]\n",
    "\n",
    "# Replace ?'s with NAs\n",
    "auto = auto.replace(\"?\",np.nan)\n",
    "\n",
    "# Convert numeric variables to numeric type\n",
    "auto = auto.assign(normalized_losses = [float(x) for x in auto.normalized_losses],bore = [float(x) for x in auto.bore],price = [float(x) for x in auto.price])\n",
    "\n",
    "# Examine the normalized_losses variable for missingness at random.\n",
    "# 1. Check a box plot for curb_weight and city_mpg when normalized losses is and isn't missing\n",
    "auto[\"missing\"] = auto.normalized_losses.isnull()\n",
    "sns.boxplot(x = \"missing\",y=\"curb_weight\",data=auto)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "sns.boxplot(x = \"missing\",y=\"city_mpg\",data=auto)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "\n",
    "# It seems as if there may be a relationship between missingness and both curb_weight city_mpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9B: Exercise 2.6.1: Question 2\n",
    "\n",
    "# 2. Perform a permutation test for the normalized_loss variable using curb_weight as the variable without missing data.\n",
    "Test_stat = auto.curb_weight[auto.missing].mean()-auto.curb_weight[-auto.missing].mean()\n",
    "Test_stat\n",
    "\n",
    "# Do a for loop to reorder the data, find the R values, and create a distribution to compare against the test statistic\n",
    "random.seed(1234)\n",
    "nrun = 1000\n",
    "R_vals = [0 for i in range(nrun)]\n",
    "ind = np.arange(len(auto))\n",
    "for i in range(nrun):\n",
    "  np.random.shuffle(ind)\n",
    "  auto[\"temp_curb_weight\"] = auto.curb_weight[ind].reset_index().iloc[:,1]\n",
    "  R_vals[i] = auto.temp_curb_weight[auto.missing].mean()-auto.temp_curb_weight[-auto.missing].mean()\n",
    "\n",
    "# Create a confidence interval\n",
    "CI = np.quantile(R_vals,[.025,.975])\n",
    "CI \n",
    "\n",
    "# Check condition\n",
    "Test_stat < CI[1] and Test_stat > CI[0]\n",
    "# normalized_losses are not missing at random\n",
    "auto = auto.drop(columns=[\"missing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9C: Exercise 2.6.1: Question 3\n",
    "\n",
    "# 3. Explain the missingness of bore with relationship to engine_type\n",
    "\n",
    "# Look at the four records where bore is missing\n",
    "pd.set_option('max_columns', None) # This will display all the columns\n",
    "auto.loc[pd.isna(auto.bore),:]\n",
    "\n",
    "# All have engine_type = rotor. Now check that this is the only time rotor appears\n",
    "auto.loc[auto.engine_type == \"rotor\",:]\n",
    "\n",
    "# When bore is missing, those are also the only incidences of the engine_type = rotor.\n",
    "# At this point an investigation of this relationship might be in order. We will elect to delete these records, but that will imply any model we build will not be able to make predictions for cars with that engine_type.\n",
    "\n",
    "auto = auto.loc[~pd.isna(auto.bore),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9D: Exercise 2.6.1: Question 4\n",
    "\n",
    "# 4. Perform mean imputation for the missing values of price. \n",
    "\n",
    "price_missing = auto.price.isnull()\n",
    "\n",
    "imp = SingleImputer(strategy={\"price\":'mean'})\n",
    "auto_mean = imp.fit_transform(auto)\n",
    "auto_mean.loc[price_missing,\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9E: Exercise 2.6.1: Question 5\n",
    "\n",
    "# 5. Perform regression imputation for the price\n",
    "\n",
    "auto_dummies = pd.get_dummies(auto.drop(columns=[\"normalized_losses\"]),drop_first=True)\n",
    "\n",
    "imp = MultipleImputer(n=1,return_list=True,strategy={\"price\":'least squares'})\n",
    "auto_reg = imp.fit_transform(auto_dummies)[0][1]\n",
    "auto_reg.loc[price_missing,\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 9F: Exercise 2.6.1: Question 6\n",
    "\n",
    "# 6. Perform imputation for both price and num_doors.\n",
    "\n",
    "doors_missing = auto.num_doors.isnull()\n",
    "\n",
    "imp = MultipleImputer(n=1,return_list=True,strategy={\"price\":'least squares',\"num_doors_two\":'binary logistic'})\n",
    "auto_reg_2 = imp.fit_transform(auto_dummies)[0][1]\n",
    "auto_reg_2.loc[doors_missing,\"num_doors_two\"]\n",
    "auto_reg_2.loc[price_missing,\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 10: Identifying outliers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "hotel_bookings = pd.read_csv(\"hotel_bookings.csv\")\n",
    "\n",
    "# full data boxplot\n",
    "sns.boxplot(y = \"stays_in_week_nights\",data=hotel_bookings)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "\n",
    "# full data histogram\n",
    "sns.displot(hotel_bookings.stays_in_week_nights,binwidth=1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 11: Looking for outliers\n",
    "\n",
    "# Try cutoffs of 3, 5, and 10, \n",
    "\n",
    "cutoff = 5\n",
    "\n",
    "hotel_trimmed = hotel_bookings.assign(zscores = (hotel_bookings.stays_in_week_nights-hotel_bookings.stays_in_week_nights.mean())/hotel_bookings.stays_in_week_nights.std())\n",
    "hotel_trimmed = hotel_trimmed.loc[hotel_trimmed.zscores < cutoff,:]\n",
    "100*(1 - len(hotel_trimmed.index)/len(hotel_bookings.index))\n",
    "sns.displot(hotel_trimmed.stays_in_week_nights,binwidth=1)\n",
    "plt.title(\"Trimmed at Zscores above \" + str(cutoff))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChunkS 12 - 15 Various methods to deal with outliers.\n",
    "# Chunk 12: 1. Log transform\n",
    "Because there are 0's, the transform is log(x+1)\n",
    "\n",
    "hotel_log = hotel_bookings.assign(logstay = np.log(hotel_bookings.stays_in_week_nights+1))\n",
    "sns.displot(hotel_log.logstay,binwidth=.5)\n",
    "plt.title(\"Log Transformation\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 13: 2. Removing outliers. This is the same as in # Chunk 11. The cutoff in this example is a z-score of 5.\n",
    "\n",
    "cutoff = 5\n",
    "hotel_trimmed = hotel_bookings.assign(zscores = (hotel_bookings.stays_in_week_nights-hotel_bookings.stays_in_week_nights.mean())/hotel_bookings.stays_in_week_nights.std())\n",
    "hotel_trimmed = hotel_trimmed.loc[hotel_trimmed.zscores < cutoff,:]\n",
    "sns.displot(hotel_trimmed.stays_in_week_nights,binwidth=1)\n",
    "plt.title(\"Trimmed at Zscores above \" + str(cutoff))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 14: 3. Change the values. In this case we can cap all observations above 6 as being equal to 6. \n",
    "\n",
    "cap =  6\n",
    "hotel_capped = hotel_bookings\n",
    "hotel_capped.stays_in_week_nights = hotel_capped.stays_in_week_nights.clip(upper=cap)\n",
    "sns.displot(hotel_capped.stays_in_week_nights,binwidth=1)\n",
    "plt.title(\"Capped at \"+str(cap))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 15: 4. Replace variable with percentiles.  \n",
    "\n",
    "hotel_bookings = pd.read_csv(\"hotel_bookings.csv\")\n",
    "\n",
    "hotel_bookings[\"perc_stay\"] = hotel_bookings.stays_in_week_nights.rank(pct=True)\n",
    "\n",
    "plt.plot(hotel_bookings.stays_in_week_nights,hotel_bookings.perc_stay,\"o\")\n",
    "plt.xlabel(\"Weeknight Stays\",y=\"Percentiles\")\n",
    "plt.title(\"Percentile Transform by Raw Data\")\n",
    "plt.show() \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 16: Use DBSCAN to find outliers based on several observations. Change eps and minPts to affect the algorithm. The number of outliers are the number of noise points and given a cluster value of -1. \n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "hotel_small = hotel_bookings.loc[:,[\"stays_in_week_nights\",\"stays_in_weekend_nights\",\"adults\"]]\n",
    "hotel_small = hotel_small.loc[0:9999,:]\n",
    "\n",
    "eps = 3\n",
    "minPts = 10\n",
    "X = hotel_small.to_numpy()\n",
    "clustering = DBSCAN(eps = eps, min_samples = minPts).fit(X)\n",
    "cluster = pd.to_numeric(clustering.labels_)\n",
    "cluster = pd.DataFrame(cluster, columns=[\"cluster\"])\n",
    "hotel_small2 = pd.concat([hotel_small,cluster],axis=1)\n",
    "hotel_small2.query(\"cluster < 0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOAModule_Notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6 (default, Jan 31 2022, 17:30:49) \n[GCC 9.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e17914ab529f92bc20a1469aaf6b9ace0664154d3788c29b41b731a15dbe8776"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
