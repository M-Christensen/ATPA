{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.21 Estimating the model generalization error with CV - Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import comb\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    RepeatedKFold,\n",
    "    LeaveOneOut,\n",
    "    LeavePOut,\n",
    "    StratifiedKFold,\n",
    "    cross_validate,\n",
    "    train_test_split\n",
    ")\n",
    "\n",
    "# load data\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y = True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "# data seems a bit imbalanced\n",
    "print(type(breast_cancer_X), type(breast_cancer_y))\n",
    "y.value_counts() / len (y)\n",
    "\n",
    "# split into training/testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.3, random_state = 0\n",
    ")\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "\n",
    "## Performing Cross-Validation\n",
    "# build a model\n",
    "logReg = LogisticRegression(\n",
    "    penalty='l2', C=10, solver='liblinear', random_state=4, max_iter=10000\n",
    ")\n",
    "\n",
    "# K-Fold CV\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "\n",
    "# Repeated K-Fold CV\n",
    "rkf = RepeatedKFold(\n",
    "    n_splits=5,\n",
    "    n_repeats=10,\n",
    "    random_state=4\n",
    ")\n",
    "\n",
    "# LOO-CV\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=4)\n",
    "\n",
    "# perform CV\n",
    "clf = cross_validate(\n",
    "    logReg,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring='accuracy', \n",
    "    return_train_score=True,\n",
    "    cv=kf # rkf # loo # skf\n",
    ")\n",
    "\n",
    "clf['test_score']\n",
    "clf['train_score']\n",
    "\n",
    "# to evaluate how overfit a model is, we can evaluate the (mean +- SD) of the training and testing scores to make sure they overlap\n",
    "print('mean train accuracy: ', np.mean(clf['train_score']), ' +- ', np.std(clf['train_score']))\n",
    "print('mean test accuracy: ', np.mean(clf['test_score']), ' +- ', np.std(clf['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.22 Cross-Validation for Hyperparameter Tuning - Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    RepeatedKFold,\n",
    "    LeaveOneOut,\n",
    "    LeavePOut,\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    train_test_split\n",
    ")\n",
    "\n",
    "## load data\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y = True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "# split into training/testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.3, random_state = 0\n",
    ")\n",
    "\n",
    "## Performing Cross-Validation\n",
    "# build a model\n",
    "logReg = LogisticRegression(\n",
    "    penalty='l2', C=10, solver='liblinear', random_state=4, max_iter=10000\n",
    ")\n",
    "\n",
    "\n",
    "# hyperparameter space\n",
    "param_grid = dict(\n",
    "    penalty = ['l1', 'l2'],\n",
    "    C=[0.1, 1, 10]\n",
    ")\n",
    "\n",
    "# K-Fold CV\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "\n",
    "# Repeated K-Fold CV\n",
    "rkf = RepeatedKFold(\n",
    "    n_splits=5,\n",
    "    n_repeats=10,\n",
    "    random_state=4\n",
    ")\n",
    "\n",
    "# LOO-CV\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=4)\n",
    "\n",
    "# search and score\n",
    "clf = GridSearchCV(\n",
    "    logReg,\n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=kf, # rkf, # loo, # skf\n",
    "    refit=True #refits the best model to the entire dataset\n",
    ")\n",
    "\n",
    "search = clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# best hyperparameters\n",
    "search.best_params_\n",
    "\n",
    "results = pd.DataFrame(search.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n",
    "print(results.shape)\n",
    "results\n",
    "\n",
    "results.sort_values(by = 'mean_test_score', ascending=False, inplace=True)\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results['mean_test_score'].plot(yerr=[results[\"std_test_score\"], results[\"std_test_score\"]], subplots=True)\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.xlabel('Hyperparameter Space')\n",
    "\n",
    "train_preds = search.predict(X_train)\n",
    "test_preds = search.predict(X_test)\n",
    "\n",
    "print('Train Accuracy: ', accuracy_score(y_train, train_preds))\n",
    "print('Test Accuracy: ', accuracy_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.24 Group Cross-Validation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"At times, our data has a grouping variable that identifies a \"\n",
    "\"    source of random variability. To account for this variability, \"\n",
    "\"    we need to update our CV scheme\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, make_scorer)\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    RepeatedKFold,\n",
    "    LeaveOneGroupOut,\n",
    "    cross_validate,\n",
    "    GridSearchCV\n",
    ")\n",
    "\n",
    "## load data\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y = True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "\n",
    "# add a group variable for demonstration\n",
    "patient_list = np.arange(10)\n",
    "X[\"patient\"] = np.random.choice(patient_list, size=len(X))\n",
    "X.head(3)\n",
    "\n",
    "# graph the number of patients in each group\n",
    "X[\"patient\"].value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train/test set\n",
    "    # leave data from one patient out - we don't patient X having data in train and test sets\n",
    "\n",
    "# start with patient 7\n",
    "train_X = X.loc[X.patient != 7,:]\n",
    "test_X = X.loc[X.patient == 7,:]\n",
    "train_Y = y.iloc[train_X.index]\n",
    "test_Y = y.iloc[test_X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression(penalty = 'l2',\n",
    "                           C = 10,\n",
    "                           solver = 'liblinear',\n",
    "                           random_state = 4, max_iter = 1000)\n",
    "\n",
    "# Group K-Fold CV - n_splits defines the value 'K'\n",
    "gkfold = GroupKFold(n_splits=5)\n",
    "\n",
    "# Leave One Group Out CV\n",
    "loGo = LeaveOneGroupOut()\n",
    "\n",
    "# Cross-Validation\n",
    "# create value to score on \n",
    "scorer = {'accuracy': make_scorer(accuracy_score),\n",
    "          'precision': 'precision'}\n",
    "clf = cross_validate(estimator = logReg,\n",
    "                    X = train_X.drop(['patient'], axis=1),\n",
    "                    y = train_Y,\n",
    "                    groups = train_X.patient,\n",
    "                    scoring = scorer,\n",
    "                    cv = gkfold.split(train_X.drop(['patient'], axis=1), train_Y, groups = train_X['patient']),\n",
    "#                     cv = loGo.split(train_X.drop(['patient'], axis=1), train_Y, groups = train_X['patient']) \n",
    "                    return_train_score = True)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00699854, 0.00408292, 0.00291872, 0.00601602, 0.00492573]),\n",
       " 'score_time': array([0.00201559, 0.00199938, 0.00208092, 0.00208998, 0.00099993]),\n",
       " 'test_accuracy': array([0.94666667, 0.9375    , 0.96226415, 0.98148148, 0.94444444]),\n",
       " 'train_accuracy': array([0.96543779, 0.96221662, 0.96277916, 0.95760599, 0.96508728]),\n",
       " 'test_precision': array([1.        , 0.95348837, 0.97142857, 0.95121951, 0.93181818]),\n",
       " 'train_precision': array([0.96319018, 0.95683453, 0.95945946, 0.95833333, 0.95774648])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf['train_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set accuracy: (0.9598, 0.9654)\n",
      "test set accuracy: (0.9387, 0.9702)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the mean +- SD metric to double check for overfitting\n",
    "train_acc_int = np.round(np.mean(clf['train_accuracy']) + np.array([-1,1]) * np.std(clf['train_accuracy']), 4)\n",
    "test_acc_int = np.round(np.mean(clf['test_accuracy']) + np.array([-1,1]) * np.std(clf['test_accuracy']), 4)\n",
    "print(\"train set accuracy: ({train1}, {train2})\".format(train1 = train_acc_int[0],\n",
    "                                                        train2 = train_acc_int[1]))\n",
    "print(\"test set accuracy: ({test1}, {test2})\".format(test1 = test_acc_int[0],\n",
    "                                                     test2 = test_acc_int[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy score 0.9627\n",
      "test accuracy score 0.9833\n"
     ]
    }
   ],
   "source": [
    "# fit the model to the entire data set\n",
    "logReg.fit(train_X.drop(['patient'], axis = 1),\n",
    "          train_Y)\n",
    "\n",
    "train_preds = logReg.predict(train_X.drop(['patient'], axis=1))\n",
    "test_preds = logReg.predict(test_X.drop(['patient'], axis=1))\n",
    "\n",
    "print(\"train accuracy score {}\".format(np.round(accuracy_score(train_Y, train_preds),4)))\n",
    "print(\"test accuracy score {}\".format(np.round(accuracy_score(test_Y, test_preds),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune Hyperparameters of a Model using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression(penalty = 'l2',\n",
    "                           C = 10,\n",
    "                           solver = 'liblinear',\n",
    "                           random_state = 4, max_iter = 1000)\n",
    "\n",
    "# Leave One Group Out CV\n",
    "loGo = LeaveOneGroupOut()\n",
    "\n",
    "# parameter grid\n",
    "param_grid = dict(penalty = ['l1', 'l2'],\n",
    "                  C = [0.1, 5, 10])\n",
    "\n",
    "# Cross-Validation\n",
    "clf = GridSearchCV(estimator = logReg,\n",
    "                   param_grid = param_grid,\n",
    "                   scoring = 'accuracy',\n",
    "#                    cv = gkfold.split(train_X.drop(['patient'], axis=1), train_Y, groups = train_X['patient']),\n",
    "                   cv = loGo.split(train_X.drop(['patient'], axis=1), train_Y, groups = train_X['patient']),\n",
    "                   refit = False)\n",
    "clf_fit = clf.fit(train_X.drop(['patient'], axis=1), train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.07444318, 0.00322991, 0.28548333, 0.00399809, 0.19246123,\n",
       "        0.00511421]),\n",
       " 'std_fit_time': array([1.49082604e-02, 6.25231497e-04, 1.00987572e-01, 7.26451302e-07,\n",
       "        2.09148801e-01, 8.74943058e-04]),\n",
       " 'mean_score_time': array([0.00100276, 0.0011132 , 0.00133883, 0.00101142, 0.00122158,\n",
       "        0.00144227]),\n",
       " 'std_score_time': array([9.29828685e-07, 3.14275792e-04, 4.69840120e-04, 2.72932937e-05,\n",
       "        4.15998466e-04, 6.90043396e-04]),\n",
       " 'param_C': masked_array(data=[0.1, 0.1, 5, 5, 10, 10],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2'],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.1, 'penalty': 'l1'},\n",
       "  {'C': 0.1, 'penalty': 'l2'},\n",
       "  {'C': 5, 'penalty': 'l1'},\n",
       "  {'C': 5, 'penalty': 'l2'},\n",
       "  {'C': 10, 'penalty': 'l1'},\n",
       "  {'C': 10, 'penalty': 'l2'}],\n",
       " 'split0_test_score': array([0.90740741, 0.92592593, 0.94444444, 0.96296296, 0.94444444,\n",
       "        0.94444444]),\n",
       " 'split1_test_score': array([0.96491228, 0.96491228, 0.96491228, 0.94736842, 0.96491228,\n",
       "        0.94736842]),\n",
       " 'split2_test_score': array([0.95918367, 0.95918367, 0.95918367, 0.95918367, 0.95918367,\n",
       "        0.95918367]),\n",
       " 'split3_test_score': array([0.88888889, 0.94444444, 0.92592593, 0.92592593, 0.92592593,\n",
       "        0.92592593]),\n",
       " 'split4_test_score': array([0.96428571, 0.94642857, 0.96428571, 0.96428571, 0.94642857,\n",
       "        0.96428571]),\n",
       " 'split5_test_score': array([0.94230769, 0.98076923, 1.        , 1.        , 1.        ,\n",
       "        1.        ]),\n",
       " 'split6_test_score': array([0.91836735, 0.91836735, 0.95918367, 0.93877551, 0.93877551,\n",
       "        0.95918367]),\n",
       " 'split7_test_score': array([0.9047619 , 0.9047619 , 0.96825397, 0.93650794, 0.96825397,\n",
       "        0.95238095]),\n",
       " 'split8_test_score': array([0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n",
       "        0.94666667]),\n",
       " 'mean_test_score': array([0.93149425, 0.94201408, 0.95772478, 0.95203816, 0.95347308,\n",
       "        0.95549327]),\n",
       " 'std_test_score': array([0.0265861 , 0.02267538, 0.02049254, 0.02132125, 0.02123496,\n",
       "        0.01895992]),\n",
       " 'rank_test_score': array([6, 5, 1, 4, 3, 2])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_fit.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.26 Nested Cross-Validation - Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, make_scorer)\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    GridSearchCV,\n",
    "    train_test_split\n",
    ")\n",
    "\n",
    "\n",
    "## load data\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y = True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30), (398,), (171,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, test_x.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the indices of the training sets because we want to index into those dataframes later\n",
    "train_x.reset_index(inplace=True, drop=True)\n",
    "train_y.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Steps For Nested CV:\n",
    "1. Separate into a training (1) and a test (2) set - test set will be used only for final validation\n",
    "2. Separate training set (1) into k-folds (1.1, 1.2, ..., 1.k)\n",
    "3. For each subsection of the training set, perform j-fold CV using the k-1 sections of the training set (1.1, 1.2, ..., 1.k-1) to train the model and evaluate using the remaining piece of the training set (1.k)\n",
    "4. Repeat for all k folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_CV(model, grid, train_x, train_y):\n",
    "    # define cross-validation schemes\n",
    "    cv_outer = KFold(n_splits=5, shuffle=True) # outer loop\n",
    "    cv_inner = KFold(n_splits=5) # 'regular' KFold CV\n",
    "\n",
    "    results = list()\n",
    "\n",
    "    for train_ix, test_ix in cv_outer.split(train_x, train_y):\n",
    "        # split into a training and a test set from the original train set\n",
    "        train_x_inner, train_y_inner = train_x.iloc[train_ix,:], train_y.iloc[train_ix]\n",
    "        test_x_inner, test_y_inner = train_x.iloc[test_ix,:], train_y.iloc[test_ix]\n",
    "\n",
    "        # j-fold CV using the K-1 sections of the original training set\n",
    "        clf = GridSearchCV(model,\n",
    "                           param_grid = grid,\n",
    "                           scoring = 'accuracy',\n",
    "                           refit = True)\n",
    "\n",
    "        # fit and evaluate\n",
    "        clf.fit(train_x_inner, train_y_inner)\n",
    "\n",
    "        preds_inner = clf.predict(train_x_inner)\n",
    "        accuracy = accuracy_score(train_y_inner, preds_inner)\n",
    "\n",
    "        results.append(accuracy)\n",
    "\n",
    "        print(\"Outer Accuracy = {OA}, Inner Accuracy = {IA}, Configuration: {CFG}\".format(OA=np.round(accuracy,3),\n",
    "                                                                                          IA=np.round(clf.best_score_,3),\n",
    "                                                                                          CFG=clf.best_params_))\n",
    "\n",
    "    print(\"Accuracy Outer = ({LOWER}, {UPPER})\".format(LOWER=np.round(np.mean(results) - np.std(results),3),\n",
    "                                                       UPPER=np.round(np.mean(results) + np.std(results),3)))\n",
    "\n",
    "    return clf.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Accuracy = 0.991, Inner Accuracy = 0.972, Configuration: {'C': 10, 'penalty': 'l1'}\n",
      "Outer Accuracy = 0.987, Inner Accuracy = 0.962, Configuration: {'C': 10, 'penalty': 'l1'}\n",
      "Outer Accuracy = 0.984, Inner Accuracy = 0.965, Configuration: {'C': 5, 'penalty': 'l1'}\n",
      "Outer Accuracy = 0.984, Inner Accuracy = 0.959, Configuration: {'C': 5, 'penalty': 'l1'}\n",
      "Outer Accuracy = 0.972, Inner Accuracy = 0.95, Configuration: {'C': 10, 'penalty': 'l2'}\n",
      "Accuracy Outer = (0.977, 0.99)\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "logReg = LogisticRegression(penalty = 'l2',\n",
    "                            C = 10,\n",
    "                            solver = 'liblinear',\n",
    "                            random_state = 4, max_iter = 1000)\n",
    "\n",
    "# parameter grid\n",
    "param_grid = dict(penalty = ['l1', 'l2'],\n",
    "                  C = [0.1, 5, 10])\n",
    "\n",
    "logReg_search = nested_CV(logReg, param_grid, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score: 0.9698\n",
      "Test Accuracy Score: 0.9298\n"
     ]
    }
   ],
   "source": [
    "# evaluate final model performance\n",
    "train_x_preds = logReg_search.predict(train_x)\n",
    "test_x_preds = logReg_search.predict(test_x)\n",
    "\n",
    "print(\"Train Accuracy Score: {}\".format(np.round(accuracy_score(train_y, train_x_preds), 4)))\n",
    "print(\"Test Accuracy Score: {}\".format(np.round(accuracy_score(test_y, test_x_preds), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Accuracy = 0.978, Inner Accuracy = 0.962, Configuration: {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 0.1, 'n_estimators': 100}\n",
      "Outer Accuracy = 0.965, Inner Accuracy = 0.95, Configuration: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 0.3, 'n_estimators': 75}\n",
      "Outer Accuracy = 0.978, Inner Accuracy = 0.956, Configuration: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 0.1, 'n_estimators': 75}\n",
      "Outer Accuracy = 0.978, Inner Accuracy = 0.962, Configuration: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 0.1, 'n_estimators': 100}\n",
      "Outer Accuracy = 0.959, Inner Accuracy = 0.956, Configuration: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 0.3, 'n_estimators': 40}\n",
      "Accuracy Outer = (0.964, 0.98)\n"
     ]
    }
   ],
   "source": [
    "# RF Model\n",
    "rf_mod = RandomForestClassifier(n_estimators = 100,\n",
    "                                criterion = 'gini',\n",
    "                                min_samples_split = 2,\n",
    "                                min_samples_leaf = 3,\n",
    "                                max_depth = 3)\n",
    "\n",
    "rf_param = dict(n_estimators = [10, 40, 75, 100, 250],\n",
    "                criterion = ['gini', 'entropy'],\n",
    "                min_samples_split = [0.1, 0.3, 0.5, 0.75, 1.0],\n",
    "                max_depth = [1, 3, 5, None])\n",
    "\n",
    "rf_search = nested_CV(rf_mod, rf_param, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate final model performance\n",
    "train_x_preds = rf_search.predict(train_x)\n",
    "test_x_preds = rf_search.predict(test_x)\n",
    "\n",
    "print(\"Train Accuracy Score: {}\".format(np.round(accuracy_score(train_y, train_x_preds), 4)))\n",
    "print(\"Test Accuracy Score: {}\".format(np.round(accuracy_score(test_y, test_x_preds), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
